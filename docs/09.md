在处理一个包含 **100 万条记录的文件**时，控制每次只读取 1000 条记录，可以通过以下方法实现。这种策略可以有效减少内存占用，避免一次性加载整个文件到内存中导致资源耗尽，同时支持流式处理和批量处理。

---

## **方法 1：使用文件流逐行读取**
Node.js 提供了 **`fs.createReadStream`** 和 **`readline`** 模块，可以逐行读取大文件并批量处理。

### 示例代码：
```javascript
const fs = require('fs');
const readline = require('readline');

async function processFile(filePath) {
    const fileStream = fs.createReadStream(filePath);

    const rl = readline.createInterface({
        input: fileStream,
        crlfDelay: Infinity, // 按行读取，支持不同系统的换行符
    });

    const batchSize = 1000;
    let batch = [];
    let count = 0;

    for await (const line of rl) {
        batch.push(line);
        count++;

        // 当达到批量大小时，处理数据
        if (batch.length === batchSize) {
            console.log(`Processing batch starting from line ${count - batchSize + 1}`);
            await processBatch(batch); // 处理当前批次
            batch = []; // 清空当前批次
        }
    }

    // 处理最后未完成的批次
    if (batch.length > 0) {
        console.log(`Processing remaining batch starting from line ${count - batch.length + 1}`);
        await processBatch(batch);
    }

    console.log('File processing completed.');
}

async function processBatch(batch) {
    // 模拟处理延迟
    await new Promise((resolve) => setTimeout(resolve, 100));
    console.log(`Processed batch of ${batch.length} records`);
}

processFile('large-file.txt');
```

### **特点**
- **优点**：
  - 流式处理，内存占用较低。
  - 支持按行读取，适合结构化文本文件（如 CSV、日志）。
- **缺点**：
  - 对于二进制文件（如图片）或非按行分割的数据，需要其他方式处理。

---

## **方法 2：按块读取文件**
通过 `fs.read` 按块读取文件，并手动解析记录。适合定长记录或二进制文件。

### 示例代码：
```javascript
const fs = require('fs');

async function processFile(filePath) {
    const fd = await fs.promises.open(filePath, 'r');
    const bufferSize = 1024 * 1024; // 每次读取 1MB 数据
    const batchSize = 1000;
    let buffer = Buffer.alloc(bufferSize);
    let bytesRead = 0;
    let remainingData = '';
    let totalRecords = 0;

    while ((bytesRead = await fd.read(buffer, 0, bufferSize, null)).bytesRead > 0) {
        const chunk = remainingData + buffer.slice(0, bytesRead).toString();
        const lines = chunk.split('\n');
        remainingData = lines.pop(); // 剩余未完整的数据行

        for (let i = 0; i < lines.length; i += batchSize) {
            const batch = lines.slice(i, i + batchSize);
            await processBatch(batch);
            totalRecords += batch.length;
        }
    }

    // 处理最后的剩余数据
    if (remainingData) {
        await processBatch([remainingData]);
        totalRecords++;
    }

    console.log(`Total records processed: ${totalRecords}`);
    await fd.close();
}

async function processBatch(batch) {
    console.log(`Processing batch of ${batch.length} records`);
    await new Promise((resolve) => setTimeout(resolve, 100)); // 模拟延迟
}

processFile('large-file.txt');
```

### **特点**
- **优点**：
  - 更灵活，适合自定义记录分隔符（如 `\n`、`,` 等）。
  - 支持部分二进制文件解析（如通过固定字节长度读取）。
- **缺点**：
  - 需要处理分块间的边界数据（如跨块的记录）。

---

## **方法 3：结合数据库批量导入**
如果文件较大且后续操作需要数据库支持，可以边读取边批量写入数据库，利用数据库的分批插入功能。

### 示例代码：
```javascript
const fs = require('fs');
const readline = require('readline');
const { Pool } = require('pg'); // 使用 PostgreSQL 示例

const pool = new Pool({
    user: 'youruser',
    host: 'localhost',
    database: 'yourdb',
    password: 'yourpassword',
    port: 5432,
});

async function processFile(filePath) {
    const fileStream = fs.createReadStream(filePath);
    const rl = readline.createInterface({
        input: fileStream,
        crlfDelay: Infinity,
    });

    const batchSize = 1000;
    let batch = [];
    let count = 0;

    for await (const line of rl) {
        batch.push(line);
        count++;

        if (batch.length === batchSize) {
            await insertBatchToDB(batch);
            batch = [];
        }
    }

    if (batch.length > 0) {
        await insertBatchToDB(batch);
    }

    console.log('File processing completed.');
    await pool.end();
}

async function insertBatchToDB(batch) {
    const client = await pool.connect();
    try {
        const values = batch.map((line) => `('${line}')`).join(',');
        await client.query(`INSERT INTO your_table (data) VALUES ${values}`);
        console.log(`Inserted batch of ${batch.length} records`);
    } finally {
        client.release();
    }
}

processFile('large-file.txt');
```

### **特点**
- **优点**：
  - 边读取边存储，适合大文件的数据持久化。
  - 数据库批量插入提高性能。
- **缺点**：
  - 依赖数据库连接和性能。

---

## **方法 4：使用 `stream.Transform` 处理流**
通过 `stream.Transform` 实现流式处理和批量管理。

### 示例代码：
```javascript
const fs = require('fs');
const { Transform } = require('stream');

class BatchTransform extends Transform {
    constructor(batchSize, options) {
        super(options);
        this.batchSize = batchSize;
        this.currentBatch = [];
    }

    _transform(chunk, encoding, callback) {
        const lines = chunk.toString().split('\n');
        for (const line of lines) {
            this.currentBatch.push(line);
            if (this.currentBatch.length === this.batchSize) {
                this.push(JSON.stringify(this.currentBatch));
                this.currentBatch = [];
            }
        }
        callback();
    }

    _flush(callback) {
        if (this.currentBatch.length > 0) {
            this.push(JSON.stringify(this.currentBatch));
        }
        callback();
    }
}

const readStream = fs.createReadStream('large-file.txt');
const batchStream = new BatchTransform(1000);
readStream.pipe(batchStream).on('data', (batch) => {
    console.log('Processing batch:', JSON.parse(batch));
});
```

### **特点**
- **优点**：
  - 强大的流式处理支持，内存占用低。
  - 适合批量处理大文件。
- **缺点**：
  - 增加了代码复杂性。

---

## **总结**
| 方法                     | 优点                                      | 适用场景                                |
|--------------------------|-------------------------------------------|-----------------------------------------|
| **文件流逐行读取**       | 简单易用，内存占用低                      | 结构化文本文件（如 CSV、日志）          |
| **按块读取文件**         | 灵活性高，适合自定义分隔符或定长记录      | 二进制文件或复杂文本文件                |
| **数据库批量导入**       | 边处理边存储，提升持久化性能              | 数据需要持久化到数据库的场景            |
| **流式处理 (`Transform`)** | 强大的批量处理能力，内存占用低           | 批量处理超大文件                       |

如果有特定场景需求，可以进一步讨论优化方案！